import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib


dataset_with_url = pd.read_csv('datasets\dataset_with_url.csv')  # Ensure the path is correct
# x is the features only hecnce why we drop the target/classifier column status 
x = dataset_with_url.drop(columns=['status'])
# y is the target/classifier column status
y = dataset_with_url['status']

# Split the dataset into training and testing sets 20% of the data will be used for testing
# and 80% for training
# stratify=y ensures that the class distribution(0s & 1s or legit and phishing) is the same in both training and testing sets
# random_state=42 ensures that the split is reproducible
X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=y
)

# This is a function to train and evaluate the model 
def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train) # fit the model to the training/testing data
    y_pred = model.predict(X_test) 
    # This part tests the model on the test set and prints the accuracy, confusion matrix, and classification report
    # The accuracy score is the percentage of correct predictions
    # The confusion matrix shows the number of true positives, true negatives, false positives, and false negatives
    # The classification report shows the precision, recall, and f1-score for each class
    print("Results for", model.__class__.__name__)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))


#This part trains the RandomForest Classifier model
# n_estimators is the number of decision trees the more trees the better the accuracy but it can slow down prediction time and training time
# max_depth is the maximum depth of the trees
rf = RandomForestClassifier(n_estimators=200, random_state=42,max_depth=20)
train_and_evaluate_model(rf, X_train, y_train, X_test, y_test)

# This part trains the Gradient Boosting Classifier model
gb = GradientBoostingClassifier(n_estimators=100, random_state=42) 
train_and_evaluate_model(gb, X_train, y_train, X_test, y_test)

# This part trains the AdaBoost Classifier model
ab = AdaBoostClassifier(n_estimators=100, random_state=42)
train_and_evaluate_model(ab, X_train, y_train, X_test, y_test)

# This part trains the Logistic Regression model
lr = LogisticRegression(max_iter=2000, random_state=42, solver='liblinear')
train_and_evaluate_model(lr, X_train, y_train, X_test, y_test)

# This part trains the K-Nearest Neighbors model
knn = KNeighborsClassifier(n_neighbors=5,weights='distance')
train_and_evaluate_model(knn, X_train, y_train, X_test, y_test)

# This part trains the Decision Tree model
dt = DecisionTreeClassifier(random_state=42, max_depth=20)
train_and_evaluate_model(dt, X_train, y_train, X_test, y_test)

# #Saves the best optimised model which is rf on this dataset.
joblib.dump(rf, "ai_model.pkl")

# # Saves the features used in the model
joblib.dump(x.columns, "features.pkl")

features = joblib.load("features.pkl")
print("Features used in the model write or save somewhere in case:",features)
